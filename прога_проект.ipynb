{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка модели"
      ],
      "metadata": {
        "id": "t1ExqFVDXNjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка предобученной"
      ],
      "metadata": {
        "id": "5MyhvJsBXSkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtKBKOQoXR6q",
        "outputId": "57943d00-b2b6-45bd-a619-9ef5790e0da6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Работаем с русскоязычной GPT от Сбера.\n",
        "# Ниже команды для загрузки и инициализации модели и токенизатора.\n",
        "model_name_or_path = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(DEVICE)"
      ],
      "metadata": {
        "id": "z1C3UxkucmMx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PYTORCH_CUDA_ALLOC_CONF = \"max_split_size_mb:512\""
      ],
      "metadata": {
        "id": "31_xLTxWgiHe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяем, как работает предуобученная"
      ],
      "metadata": {
        "id": "NwIGY5AZc64O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt engineering for QA\n",
        "text = \"Вопрос: 'Сколько будет 2+2?'\\nОтвет:\" \n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "out = model.generate(input_ids, do_sample=False) \n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRAorsJdctZG",
        "outputId": "85bc6499-377c-4991-ebeb-57e166a657c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вопрос: 'Сколько будет 2+2?'\n",
            "Ответ: '2+2=4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Составляем данные, на которых будет учиться модель"
      ],
      "metadata": {
        "id": "M_uvuPpZGfyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re"
      ],
      "metadata": {
        "id": "4TorjJS8rHJE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = requests.session()\n",
        "adapter = requests.adapters.HTTPAdapter(\n",
        "    pool_connections=10, # Количество пулов соединений\n",
        "    pool_maxsize=10 # Размер каждого пула\n",
        ")\n",
        "session.mount('https://', adapter)\n",
        "response = session.get('https://killpls.me')"
      ],
      "metadata": {
        "id": "BQhEbvMEpxXv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def page_to_dict(page):\n",
        "    texts_list = []\n",
        "    texts = page.findAll('div', class_='col-xs-12',style='margin:0.5em 0;line-height:1.785em')\n",
        "    id_date_tags = page.findAll('div', class_='col-xs-6')\n",
        "    for ct in range(len(texts)):\n",
        "        temp_dict = {}\n",
        "        # id_text\n",
        "        text = texts[ct].text.replace('\\n','').replace('\\r','')\n",
        "        text = texts[ct].text.replace('\\t','')\n",
        "        text = text.replace('        ','').replace('    ','')\n",
        "        # tags\n",
        "        tags = re.findall(r'(?<=>)[а-я]*(?=</a>)', str(id_date_tags[ct*2+1]))\n",
        "        temp_dict['tags'] = tags\n",
        "        temp_dict['text'] = text\n",
        "        texts_list.append(temp_dict)\n",
        "    return texts_list"
      ],
      "metadata": {
        "id": "NxgDa5WEpxdN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "req = session.get(f'https://killpls.me/page/1000')\n",
        "page = BeautifulSoup(req.text, 'html.parser')\n",
        "page_to_dict(page)"
      ],
      "metadata": {
        "id": "wyLuccAuayWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_all = []\n",
        "errors = []\n",
        "for count_number in tqdm.trange(100,200):\n",
        "    req = session.get(f'https://killpls.me/page/{str(count_number)}')\n",
        "    try:\n",
        "        page = BeautifulSoup(req.text, 'html.parser')\n",
        "        list_all.append(page_to_dict(page))\n",
        "    except:\n",
        "        errors.append(count_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd0n1DQ2qiOV",
        "outputId": "93be1f52-cd13-443d-ac63-ba829ea458f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_out = []\n",
        "count = {}\n",
        "for item_list in list_all:\n",
        "    for item in item_list:\n",
        "        if item['text'] != '18+':\n",
        "            list_out.append(item[\"text\"])\n",
        "        "
      ],
      "metadata": {
        "id": "bVdfryAZlSwO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/corpus_kmp.txt', 'w', encoding='utf-8') as f:\n",
        "    #string_data = ''\n",
        "    #print('[',file=f)\n",
        "    for item in list_out:\n",
        "        if '18+' not in item:\n",
        "            item = item.replace('\\n','')\n",
        "            item = re.sub(r'\\.{2,5}', '.',item)\n",
        "            item = re.sub(r'(?<=[А-я])\\.(?=[А-я])', '. ',item)\n",
        "            print(item,file=f)\n",
        "    #print(']',file=f)"
      ],
      "metadata": {
        "id": "ggjgiAbMqiRp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Файнтюнинг"
      ],
      "metadata": {
        "id": "-XRIjPv4eViT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DKIQSODOisUq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "train_path = '/content/corpus_kmp.txt'\n",
        "# Создание датасета\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=train_path,block_size=8)\n",
        "  \n",
        "# Создание даталодера (нарезает текст на оптимальные по длине куски)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, \n",
        "                                                mlm=False)"
      ],
      "metadata": {
        "id": "QhSAJhGUfEbO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned\", # The output directory\n",
        "    overwrite_output_dir=True, # Overwrite the content of the output dir\n",
        "    num_train_epochs=5, # number of training epochs\n",
        "    per_device_train_batch_size=16, # batch size for training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    warmup_steps=10, # number of warmup steps for learning rate scheduler\n",
        "    gradient_accumulation_steps=16, # to make \"virtual\" batch size larger\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
        ")"
      ],
      "metadata": {
        "id": "GFrKCjcAfXcv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "Z48cXCjIffoq",
        "outputId": "9ecde25e-7329-4287-e967-aee893c0d39f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 15:59, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=245, training_loss=4.409580277423469, metrics={'train_runtime': 964.1435, 'train_samples_per_second': 65.685, 'train_steps_per_second': 0.254, 'total_flos': 2046275715465216.0, 'train_loss': 4.409580277423469, 'epoch': 4.95})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/proga_temp/model_new.pt')"
      ],
      "metadata": {
        "id": "zdzaXjgvfijS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
        "#output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "#output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
        "\n",
        "torch.save(model.state_dict(), f'/content/drive/MyDrive/proga_temp/{WEIGHTS_NAME}')\n",
        "model.config.to_json_file(f'/content/drive/MyDrive/proga_temp/{CONFIG_NAME}')\n",
        "tokenizer.save_pretrained(f'/content/drive/MyDrive/proga_temp/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NdVQr632kti",
        "outputId": "27456bc0-a0a4-41d9-89c6-deb1336bb4c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/proga_temp/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/proga_temp/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/proga_temp/vocab.json',\n",
              " '/content/drive/MyDrive/proga_temp/merges.txt',\n",
              " '/content/drive/MyDrive/proga_temp/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Я очень хочу спать\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(input_ids, \n",
        "                        do_sample=True,\n",
        "                        num_beams=2,\n",
        "                        temperature=1.5,\n",
        "                        top_p=0.9,\n",
        "                        max_length=200,\n",
        "                        )\n",
        "\n",
        "generated_text = list(map(tokenizer.decode, out))[0]\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y82Bb706n44Q",
        "outputId": "09666604-e50d-4b88-a4e7-3e02fe1c1d0c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Я очень хочу спать. Не могу уснуть, постоянно ворочаюсь. В голову лезет всякая фигня. ПМП. Захожу в комнату, а там на диване лежит моя лучшая подруга. Она в одной ночнушке, в другой - в трусах. И я не знаю, что делать. У неё на коленях куча денег, и она говорит, что это всё она заработала и теперь она может тратить их на всякую ерунду. Мне дико стыдно, что я не могу дать ей ни копейки. А она заявляет, что это её зарплата. ПМП. В итоге я в бешенстве от всего этого, но в то же время рад, что она меня не бросила. ПМП. ПМП. ПМП. ПМП. Блядь. ПМП. БЛЯТЬ. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП. ПМП\n"
          ]
        }
      ]
    }
  ]
}